{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2021/22**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Recommender Systems\n",
    "This lecture is about recommender systems (or recommendation systems). In the meantime, we highlight the usefulness of Spark SQL, particularly when it relates to persistent tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "As mentioned in the initial lectures, Spark SQL is a Spark module for structured data processing. It works alongside the APIs of DataFrame and Dataset and it is responsible for performing extra optimizations. We can also execute SQL queries and reading data from various files formats an Hive tables. (Apache Hive can manage large datasets residing in distributed storage using SQL)\n",
    "\n",
    "Further details can be found in https://spark.apache.org/docs/latest/sql-programming-guide.html  and https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "We can check the reference guide for Structured Query Language (SQL) which includes syntax, semantics, keywords, and examples for common SQL usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Problem formulation\n",
    "\n",
    "This exercise aims to build a recommender system of books, with focus on the recommendation model\n",
    "itself.\n",
    "The functional requirements for the Spark program we want to create are as follows:\n",
    "1. To load the dataset and perform exploratory analysis, then store the information properly cleaned, including as SQL tables.\n",
    "2. To create a recommendation model supported by the ALS algorithm provided by Spark MLlib.\n",
    "3. To pre-compute recommendations and store them in SQL tables.\n",
    "4. To show recommendations.\n",
    "\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "\n",
    "The data we are processing is from the dataset **Book-Crossing**. As stated in the website from where it can be downloaded, http://www2.informatik.uni-freiburg.de/~cziegler/BX/ , the BookCrossing (BX) dataset was collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. It contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.\n",
    "\n",
    "Alternatively, we can use the command *wget* from the Terminal to download the dataset:\n",
    "\n",
    "    wget http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip\n",
    "\n",
    "\n",
    "The dataset comprises 3 tables, as follows:\n",
    "- **BX-Users**. Contains the users. Note that user IDs ( User-ID ) have been anonymized and map to integers. Demographic data is provided ( Location , Age ) if available. Otherwise, these fields contain NULL-values.\n",
    "- **BX-Books**. Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given ( Book-Title , Book-Author , Year-Of-Publication , Publisher ), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours ( Image-URL-S , Image-URL-M , Image-URL-L ), i.e., small, medium, large. These URLs point to the Amazon web site.\n",
    "- **BX-Book-Ratings**. Contains the book rating information. Ratings ( Book-Rating ) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "The columns are separated by ; and all files contain the correspondent header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to install some packages, e.g. matplotlib\n",
    "\n",
    "# ! pip3 install matplotlib\n",
    "# ! pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.158170Z",
     "start_time": "2021-03-07T19:11:07.859222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some imports \n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Useful visualization functions\n",
    "\n",
    "Some functions that we can use to plot data but as Python dataframes.\n",
    "\n",
    "**Disclaimer**: these functions are broadly distributed among users. Further adjustments are needed and/or advisable. Feel free to use your own plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, xcol, ycol):\n",
    "    sns.lineplot(data=df, x=xcol, y=ycol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistogram(df, xcol, huecol, bins):\n",
    "    if huecol:\n",
    "        sns.histplot(data=df, x=xcol, hue=huecol, multiple=\"stack\")\n",
    "    else:\n",
    "        sns.histplot(data=df, x=xcol, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatter(df, xcol, ycol, huecol):\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.scatterplot(data=df, x=xcol, y=ycol, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterMatrix(df, huecol):\n",
    "    sns.pairplot(data=df, hue=huecol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Collect and label data\n",
    "\n",
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.953150Z",
     "start_time": "2021-03-07T19:11:08.185863Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/saltedcookie/Desktop/aulasABD/Aula_07\n",
      "total 117996\n",
      "drwxrwxr-x 5 saltedcookie saltedcookie     4096 abr  7 15:24 .\n",
      "drwxrwxr-x 8 saltedcookie saltedcookie     4096 abr  7 14:41 ..\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie 30682276 out 11  2004 BX-Book-Ratings.csv\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie 77787439 out 11  2004 BX-Books.csv\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie 12284157 out 11  2004 BX-Users.csv\n",
      "drwxrwxr-x 3 saltedcookie saltedcookie     4096 abr  6 20:16 data\n",
      "drwxrwxr-x 2 saltedcookie saltedcookie     4096 abr  7 14:44 .ipynb_checkpoints\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie    38434 abr  7 14:50 Recommender.ipynb\n",
      "drwxrwxr-x 4 saltedcookie saltedcookie     4096 abr  7 14:41 .svn\n"
     ]
    }
   ],
   "source": [
    "! pwd \n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"User-ID\";\"Location\";\"Age\"\n",
      "\"1\";\"nyc, new york, usa\";NULL\n",
      "\"278857\";\"knoxville, tennessee, usa\";NULL\n",
      "\"278858\";\"dublin, n/a, ireland\";NULL\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 BX-Users.csv\n",
    "! tail -n 2 BX-Users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ISBN\";\"Book-Title\";\"Book-Author\";\"Year-Of-Publication\";\"Publisher\";\"Image-URL-S\";\"Image-URL-M\";\"Image-URL-L\"\n",
      "\"0195153448\";\"Classical Mythology\";\"Mark P. O. Morford\";\"2002\";\"Oxford University Press\";\"http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg\"\n",
      "\"0192126040\";\"Republic (World's Classics)\";\"Plato\";\"1996\";\"Oxford University Press\";\"http://images.amazon.com/images/P/0192126040.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0192126040.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0192126040.01.LZZZZZZZ.jpg\"\n",
      "\"0767409752\";\"A Guided Tour of Rene Descartes' Meditations on First Philosophy with Complete Translations of the Meditations by Ronald Rubin\";\"Christopher  Biffle\";\"2000\";\"McGraw-Hill Humanities/Social Sciences/Languages\";\"http://images.amazon.com/images/P/0767409752.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0767409752.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0767409752.01.LZZZZZZZ.jpg\"\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 BX-Books.csv\n",
    "! tail -n 2 BX-Books.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"User-ID\";\"ISBN\";\"Book-Rating\"\n",
      "\"276725\";\"034545104X\";\"0\"\n",
      "\"276721\";\"0590442449\";\"10\"\n",
      "\"276723\";\"05162443314\";\"8\"\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 BX-Book-Ratings.csv\n",
    "! tail -n 2 BX-Book-Ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.961000Z",
     "start_time": "2021-03-07T19:11:08.954809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# some Spark related imports we will use hereafter\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.480883Z",
     "start_time": "2021-03-07T19:11:12.479044Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build a SparkSession instance if one does not exist. Notice that we can only have one per JVM\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Recommender\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.476668Z",
     "start_time": "2021-03-07T19:11:08.962435Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read the dataset \n",
    "\n",
    "df_raw_users = spark.read.csv(\"BX-Users.csv\", header=\"true\", \n",
    "                              inferSchema=\"true\", sep=\";\")\n",
    "\n",
    "df_raw_books = spark.read.csv(\"BX-Books.csv\", header=\"true\", inferSchema=\"true\", sep=\";\")\n",
    "\n",
    "df_raw_ratings = spark.read.csv(\"BX-Book-Ratings.csv\", header=\"true\", inferSchema=\"true\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:13.028845Z",
     "start_time": "2021-03-07T19:11:12.483086Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------------------\n",
      " User-ID  | 1                         \n",
      " Location | nyc, new york, usa        \n",
      " Age      | NULL                      \n",
      "-RECORD 1-----------------------------\n",
      " User-ID  | 2                         \n",
      " Location | stockton, california, usa \n",
      " Age      | 18                        \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "278859"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check users - schema and count\n",
    "\n",
    "df_raw_users.printSchema()\n",
    "df_raw_users.show(2, vertical=True, truncate=False) \n",
    "num_users = df_raw_users.count()\n",
    "num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Title: string (nullable = true)\n",
      " |-- Book-Author: string (nullable = true)\n",
      " |-- Year-Of-Publication: integer (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Image-URL-S: string (nullable = true)\n",
      " |-- Image-URL-M: string (nullable = true)\n",
      " |-- Image-URL-L: string (nullable = true)\n",
      "\n",
      "-RECORD 0---------------------------------------------------------------------------\n",
      " ISBN                | 0195153448                                                   \n",
      " Book-Title          | Classical Mythology                                          \n",
      " Book-Author         | Mark P. O. Morford                                           \n",
      " Year-Of-Publication | 2002                                                         \n",
      " Publisher           | Oxford University Press                                      \n",
      " Image-URL-S         | http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg \n",
      " Image-URL-M         | http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg \n",
      " Image-URL-L         | http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg \n",
      "-RECORD 1---------------------------------------------------------------------------\n",
      " ISBN                | 0002005018                                                   \n",
      " Book-Title          | Clara Callan                                                 \n",
      " Book-Author         | Richard Bruce Wright                                         \n",
      " Year-Of-Publication | 2001                                                         \n",
      " Publisher           | HarperFlamingo Canada                                        \n",
      " Image-URL-S         | http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg \n",
      " Image-URL-M         | http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg \n",
      " Image-URL-L         | http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "271379"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check books - schema and count\n",
    "\n",
    "df_raw_books.printSchema()\n",
    "df_raw_books.show(2, vertical=True, truncate=False)\n",
    "num_books = df_raw_books.count()\n",
    "\n",
    "num_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: integer (nullable = true)\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Rating: integer (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------\n",
      " User-ID     | 276725     \n",
      " ISBN        | 034545104X \n",
      " Book-Rating | 0          \n",
      "-RECORD 1-----------------\n",
      " User-ID     | 276726     \n",
      " ISBN        | 0155061224 \n",
      " Book-Rating | 5          \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1149780"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check ratings - schema and count\n",
    "\n",
    "df_raw_ratings.printSchema()\n",
    "df_raw_ratings.show(2, vertical=True, truncate=False)\n",
    "num_ratings = df_raw_ratings.count()\n",
    "\n",
    "num_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no reasons to drop any column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Evaluate data\n",
    "\n",
    "Let us get some data insight, with some exploratory data analysis based on descriptive statistics and visualizations if advisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          User-ID|\n",
      "+-------+-----------------+\n",
      "|  count|           278859|\n",
      "|   mean|         139429.5|\n",
      "| stddev|80499.51502027822|\n",
      "|    min|  , milan, italy\"|\n",
      "|    max|            99999|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|            Location|\n",
      "+-------+--------------------+\n",
      "|  count|              278859|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|\"alexandria\"., \"a...|\n",
      "|    max|��������� �,�����...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|            278857|\n",
      "|   mean| 34.75143370454978|\n",
      "| stddev|14.428097382455421|\n",
      "|    min|                 0|\n",
      "|    max|              NULL|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some column statistics, one by one, using describe\n",
    "\n",
    "for cl in df_raw_users.columns:\n",
    "    df_raw_users.describe(cl).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                ISBN|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|1.0412353852604752E9|\n",
      "| stddev|1.4877523499740589E9|\n",
      "|    min|          0000913154|\n",
      "|    max|          B0002K6K8O|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|          Book-Title|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|            Infinity|\n",
      "| stddev|                 NaN|\n",
      "|    min| A Light in the S...|\n",
      "|    max|   �?�?thique en toc|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------+\n",
      "|summary|   Book-Author|\n",
      "+-------+--------------+\n",
      "|  count|        271379|\n",
      "|   mean|          null|\n",
      "| stddev|          null|\n",
      "|    min|      D. Chiel|\n",
      "|    max|�?�?ric Holder|\n",
      "+-------+--------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|Year-Of-Publication|\n",
      "+-------+-------------------+\n",
      "|  count|             271379|\n",
      "|   mean| 1959.7560496574902|\n",
      "| stddev|  258.0113625638112|\n",
      "|    min|                  0|\n",
      "|    max|               2050|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|           Publisher|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|  25032.333333333332|\n",
      "| stddev|   21676.03850645531|\n",
      "|    min| Editions P. Terrail|\n",
      "|    max|   �?�?ditions 10/18|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|         Image-URL-S|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|http://images.ama...|\n",
      "|    max|http://images.ama...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|         Image-URL-M|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|http://images.ama...|\n",
      "|    max|http://images.ama...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|         Image-URL-L|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|http://images.ama...|\n",
      "|    max|http://images.ama...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for cl in df_raw_books.columns:\n",
    "    df_raw_books.describe(cl).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           User-ID|\n",
      "+-------+------------------+\n",
      "|  count|           1149780|\n",
      "|   mean|140386.39512602412|\n",
      "| stddev|  80562.2777185127|\n",
      "|    min|                 2|\n",
      "|    max|            278854|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-----------+\n",
      "|summary|       ISBN|\n",
      "+-------+-----------+\n",
      "|  count|    1149780|\n",
      "|   mean|   Infinity|\n",
      "| stddev|        NaN|\n",
      "|    min| 0330299891|\n",
      "|    max| �423350229|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|       Book-Rating|\n",
      "+-------+------------------+\n",
      "|  count|           1149780|\n",
      "|   mean|2.8669501991685364|\n",
      "| stddev|3.8541838592016617|\n",
      "|    min|                 0|\n",
      "|    max|                10|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for cl in df_raw_ratings.columns:\n",
    "    df_raw_ratings.describe(cl).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T11:54:16.056821Z",
     "start_time": "2021-03-07T11:54:16.050981Z"
    },
    "hidden": true
   },
   "source": [
    "Following previous understanding, all collected data should be considered as of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now we have to prepare data in a way that it can be properly used by ML algorithms, which includes selection and extraction of features, as well as dealing with poor data quality if that is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data cleasing\n",
    "\n",
    "We will look at\n",
    "* Data types\n",
    "* Nulls\n",
    "* Extreme values e.g. outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[278859, 278857]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_users, df_raw_users.dropna().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[271379, 271379]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_books, df_raw_books.dropna().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1149780, 1149780]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_ratings, df_raw_ratings.dropna().count()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only differences are spotted for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----+\n",
      "|        User-ID|  Location| Age|\n",
      "+---------------+----------+----+\n",
      "|         275081|cernusco s|null|\n",
      "|, milan, italy\"|      NULL|null|\n",
      "+---------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_users.filter(column(\"Age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can conclude that:\n",
    "\n",
    "- User-ID is set as string in users but integer in ratings\n",
    "- Age is set as string in users, with range of values between 0 to null\n",
    "- Year-Of_Publication ranges from 0 to 2050\n",
    "- Book-rating ranges from 0 to 10\n",
    "- Only two observations in users hold null values\n",
    "\n",
    "What can we do now about nulls, data types or extreme values? \n",
    "\n",
    "Recall that if we delete an observation in one table, still consistency among tables has to be preserved. We leave it as exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          User-ID|\n",
      "+-------+-----------------+\n",
      "|  count|           278859|\n",
      "|   mean|         139429.5|\n",
      "| stddev|80499.51502027822|\n",
      "|    min|  , milan, italy\"|\n",
      "|    25%|          69704.0|\n",
      "|    50%|         139408.0|\n",
      "|    75%|         209127.0|\n",
      "|    max|            99999|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|            Location|\n",
      "+-------+--------------------+\n",
      "|  count|              278859|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|\"alexandria\"., \"a...|\n",
      "|    25%|                null|\n",
      "|    50%|                null|\n",
      "|    75%|                null|\n",
      "|    max|��������� �,�����...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|            278857|\n",
      "|   mean| 34.75143370454978|\n",
      "| stddev|14.428097382455421|\n",
      "|    min|                 0|\n",
      "|    25%|              24.0|\n",
      "|    50%|              32.0|\n",
      "|    75%|              44.0|\n",
      "|    max|              NULL|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outliers: for that, we use summary(), one column by one, using summary\n",
    "\n",
    "for cl in df_raw_users.columns:\n",
    "    df_raw_users.select(cl).summary().show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                ISBN|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|1.0412353852604752E9|\n",
      "| stddev|1.4877523499740589E9|\n",
      "|    min|          0000913154|\n",
      "|    25%|        3.85033516E8|\n",
      "|    50%|        6.71474308E8|\n",
      "|    75%|         8.8677996E8|\n",
      "|    max|          B0002K6K8O|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|          Book-Title|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|            Infinity|\n",
      "| stddev|                 NaN|\n",
      "|    min| A Light in the S...|\n",
      "|    25%|                50.0|\n",
      "|    50%|              1876.0|\n",
      "|    75%|              1984.0|\n",
      "|    max|   �?�?thique en toc|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------+\n",
      "|summary|   Book-Author|\n",
      "+-------+--------------+\n",
      "|  count|        271379|\n",
      "|   mean|          null|\n",
      "| stddev|          null|\n",
      "|    min|      D. Chiel|\n",
      "|    25%|          null|\n",
      "|    50%|          null|\n",
      "|    75%|          null|\n",
      "|    max|�?�?ric Holder|\n",
      "+-------+--------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|Year-Of-Publication|\n",
      "+-------+-------------------+\n",
      "|  count|             271379|\n",
      "|   mean| 1959.7560496574902|\n",
      "| stddev|  258.0113625638112|\n",
      "|    min|                  0|\n",
      "|    25%|               1989|\n",
      "|    50%|               1995|\n",
      "|    75%|               2000|\n",
      "|    max|               2050|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|           Publisher|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|  25032.333333333332|\n",
      "| stddev|   21676.03850645531|\n",
      "|    min| Editions P. Terrail|\n",
      "|    25%|                 3.0|\n",
      "|    50%|             37547.0|\n",
      "|    75%|             37547.0|\n",
      "|    max|   �?�?ditions 10/18|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|         Image-URL-S|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|http://images.ama...|\n",
      "|    25%|                null|\n",
      "|    50%|                null|\n",
      "|    75%|                null|\n",
      "|    max|http://images.ama...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|         Image-URL-M|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|http://images.ama...|\n",
      "|    25%|                null|\n",
      "|    50%|                null|\n",
      "|    75%|                null|\n",
      "|    max|http://images.ama...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|         Image-URL-L|\n",
      "+-------+--------------------+\n",
      "|  count|              271379|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|http://images.ama...|\n",
      "|    25%|                null|\n",
      "|    50%|                null|\n",
      "|    75%|                null|\n",
      "|    max|http://images.ama...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cl in df_raw_books.columns:\n",
    "    df_raw_books.select(cl).summary().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           User-ID|\n",
      "+-------+------------------+\n",
      "|  count|           1149780|\n",
      "|   mean|140386.39512602412|\n",
      "| stddev|  80562.2777185127|\n",
      "|    min|                 2|\n",
      "|    25%|             70314|\n",
      "|    50%|            141010|\n",
      "|    75%|            211041|\n",
      "|    max|            278854|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------+\n",
      "|summary|        ISBN|\n",
      "+-------+------------+\n",
      "|  count|     1149780|\n",
      "|   mean|    Infinity|\n",
      "| stddev|         NaN|\n",
      "|    min|  0330299891|\n",
      "|    25%|3.75508627E8|\n",
      "|    50%|5.15093025E8|\n",
      "|    75%|7.86905239E8|\n",
      "|    max|  �423350229|\n",
      "+-------+------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|       Book-Rating|\n",
      "+-------+------------------+\n",
      "|  count|           1149780|\n",
      "|   mean|2.8669501991685364|\n",
      "| stddev|3.8541838592016617|\n",
      "|    min|                 0|\n",
      "|    25%|                 0|\n",
      "|    50%|                 0|\n",
      "|    75%|                 7|\n",
      "|    max|                10|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cl in df_raw_ratings.columns:\n",
    "    df_raw_ratings.select(cl).summary().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110761"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_raw_users.select(col(\"Age\")).where(col(\"Age\") == 'NULL').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of NULLs (as string) in the Age column. So, we may\n",
    "- replace the NULLs with the average of others for example (with Imputer) \n",
    "- drop the column Age in case we can live without it \n",
    "- delete the records with NULL in the column Age\n",
    "\n",
    "It is for further discussion!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We drop column Age now\n",
    "    \n",
    "df_raw_users = df_raw_users.drop(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us carry out further checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Year-Of-Publication|\n",
      "+-------------------+\n",
      "|               2005|\n",
      "|               2006|\n",
      "|               2008|\n",
      "|               2010|\n",
      "|               2011|\n",
      "|               2012|\n",
      "|               2020|\n",
      "|               2021|\n",
      "|               2024|\n",
      "|               2026|\n",
      "|               2030|\n",
      "|               2037|\n",
      "|               2038|\n",
      "|               2050|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2004 was when data was collected\n",
    "\n",
    "( df_raw_books\n",
    "     .select('Year-Of-Publication')\n",
    "     .where(col('Year-Of-Publication')>2004)\n",
    "     .distinct()\n",
    "     .orderBy('Year-Of-Publication')\n",
    "     .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Year-Of-Publication|\n",
      "+-------------------+\n",
      "|                  0|\n",
      "|               1376|\n",
      "|               1378|\n",
      "|               1806|\n",
      "|               1897|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prior to 1900\n",
    "\n",
    "( df_raw_books\n",
    "     .select('Year-Of-Publication')\n",
    "     .where(col('Year-Of-Publication')<1900)\n",
    "     .distinct()\n",
    "     .orderBy('Year-Of-Publication')\n",
    "     .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot year of publication\n",
    "\n",
    "df_plot = df_raw_books.select(\"Year-Of-Publication\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHistogram(df_plot, 'Year-Of-Publication', \"\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop some columns anyway\n",
    "\n",
    "df_raw_books = df_raw_books.drop('Year-Of-Publication', \n",
    "                         'Image-URL-S', 'Image-URL-M', 'Image-URL-L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T11:17:04.469058Z",
     "start_time": "2021-03-07T11:17:04.465458Z"
    },
    "hidden": true
   },
   "source": [
    "## Saving clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we may want to have a smaller dataset just for the purpose of testing locally.\n",
    "But in this case, as mentioned above, consistency among the three tables has to be guaranteed. \n",
    "\n",
    "Let us try to use just the normal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users\n",
    "\n",
    "output_users = \"users.parquet\"\n",
    "df_raw_users.write.mode(\"overwrite\").parquet(output_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 15:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/04/07 15:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/04/07 15:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/04/07 15:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/04/07 15:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/04/07 15:50:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/04/07 15:50:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/04/07 15:50:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/04/07 15:50:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Books\n",
    "\n",
    "output_books = \"books.parquet\"\n",
    "df_raw_books.write.mode(\"overwrite\").parquet(output_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 15:50:21 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Ratings\n",
    "\n",
    "output_ratings = \"ratings.parquet\"\n",
    "df_raw_ratings.write.mode(\"overwrite\").parquet(output_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 118040\r\n",
      "drwxrwxr-x 8 saltedcookie saltedcookie     4096 abr  7 15:50 .\r\n",
      "drwxrwxr-x 8 saltedcookie saltedcookie     4096 abr  7 14:41 ..\r\n",
      "drwxr-xr-x 2 saltedcookie saltedcookie     4096 abr  7 15:50 books.parquet\r\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie 30682276 out 11  2004 BX-Book-Ratings.csv\r\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie 77787439 out 11  2004 BX-Books.csv\r\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie 12284157 out 11  2004 BX-Users.csv\r\n",
      "drwxrwxr-x 3 saltedcookie saltedcookie     4096 abr  6 20:16 data\r\n",
      "drwxrwxr-x 2 saltedcookie saltedcookie     4096 abr  7 14:44 .ipynb_checkpoints\r\n",
      "drwxr-xr-x 2 saltedcookie saltedcookie     4096 abr  7 15:50 ratings.parquet\r\n",
      "-rw-rw-r-- 1 saltedcookie saltedcookie    71893 abr  7 15:50 Recommender.ipynb\r\n",
      "drwxrwxr-x 4 saltedcookie saltedcookie     4096 abr  7 14:41 .svn\r\n",
      "drwxr-xr-x 2 saltedcookie saltedcookie     4096 abr  7 15:50 users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Check in the running directory if that was accomplished\n",
    "\n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, save them as persistent tables into Hive metastore\n",
    "\n",
    "Notice\n",
    "- An existing Hive deployment is not necessary to use this feature. Spark will take care of it.\n",
    "- We can create a SQL table from a DataFrame with createOrReplaceTempView command, valid for the session. (there is also the option of global temporary views, to be shared among all sessions till the Spark application terminates)\n",
    "- But with saveAsTable, there will be a pointer to the data in the Hive metastore. So persistent tables will exist even after the Spark program has restarted, as long as connection is maintained to the same metastore.\n",
    "\n",
    "See details in http://spark.apache.org/docs/latest/sql-data-sources.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 15:53:01 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/04/07 15:53:01 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/04/07 15:53:03 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/04/07 15:53:03 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore saltedcookie@127.0.1.1\n",
      "22/04/07 15:53:03 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "22/04/07 15:53:03 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "22/04/07 15:53:03 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "22/04/07 15:53:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/04/07 15:53:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/04/07 15:53:04 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Persistent tables into Hive metastore\n",
    "\n",
    "df_raw_users.write.mode(\"overwrite\").saveAsTable(\"UsersTable\")\n",
    "df_raw_books.write.mode(\"overwrite\").saveAsTable(\"BooksTable\")\n",
    "df_raw_ratings.write.mode(\"overwrite\").saveAsTable(\"RatingsTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to be used hereafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a smaller dataset, once properly built\n",
    "\n",
    "# df_clean_users = ...\n",
    "df_clean_users = df_raw_users\n",
    "df_clean_books = df_raw_books\n",
    "df_clean_ratings = df_raw_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete memory consuming variables that are no longer needed\n",
    "\n",
    "del df_raw_users, df_raw_books, df_raw_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T21:59:55.649596Z",
     "start_time": "2021-03-06T21:59:55.646188Z"
    },
    "hidden": true
   },
   "source": [
    "## Final  overview\n",
    "After establishing the clean data to be used, we should get an overview about what we have achieved, with some statistics and visualizations.\n",
    "\n",
    "**But** \n",
    "\n",
    "we leave it as it is now, because so far there are no significant changes (we just drop columns). Eventually, we could check the ratings and draw some plots, as it is the critical part of the system. You can have a go in that regard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T13:53:01.240297Z",
     "start_time": "2021-03-06T13:53:01.235639Z"
    },
    "hidden": true
   },
   "source": [
    "## Features transformation\n",
    "\n",
    "As mentioned, ratings are critial here. Recall that, in the dataframe, the schema is User-ID (integer), ISBN (string) and Book-rating (integer). ISBN poses a problem as the ML algorithm requires numbers to process. Hence, we have to convert it to numbers - we will use `StringIndexer` to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringerIndexer for ISBN\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"ISBN-Index\", handleInvalid=\"keep\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from ratings that are going to be considered in the model\n",
    "\n",
    "user_col = \"User-ID\"\n",
    "item_col = \"ISBN-Index\" \n",
    "rating_col = \"Book-Rating\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T12:46:42.492329Z",
     "start_time": "2021-03-06T12:46:42.487767Z"
    },
    "hidden": true
   },
   "source": [
    "# Select and train model\n",
    "\n",
    "In order to create the recommendation model, we will use the Alternating Least Squares (ALS) algorithm provided by Spark MLlib. See details in http://spark.apache.org/docs/latest/ml-collaborative-filtering.html , as we advise to check the main assumptions the implemented algorithm relies upon. For example, notice that:\n",
    "- it underlies a collaborative filtering strategy;\n",
    "- it aims to fill in the missing entries of a user-item association matrix, in which users and items are described by a small set of latent factors that can be used to predict missing entries. The latent factors are learned by the ALS algorithm.\n",
    "\n",
    "Again, as for data to train the model, the focus is on ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Train/test split\n",
    "\n",
    "We will use the standard split 80/20, for the reasons explained in previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:31.983392Z",
     "start_time": "2021-03-07T19:11:30.973303Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 919995 rows in the training set and 229785 in the test set.\n"
     ]
    }
   ],
   "source": [
    "# train/test clean ratings split\n",
    "\n",
    "df_train, df_test = df_clean_ratings.randomSplit([0.8, 0.2], 42)\n",
    "\n",
    "# caching data ... but just the training part\n",
    "df_train.cache()\n",
    "\n",
    "# print the number of rows in each part\n",
    "print(f\"There are {df_train.count()} rows in the training set and {df_test.count()} in the test set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** \n",
    "\n",
    "As we did with clean data, we may consider storing the data split into files, should we want to use it elsewhere. \n",
    "This relates to the need of guaranteeing unicity in a different environment. \n",
    "We leave it as it is now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## ALS model\n",
    "\n",
    "Using the `ALS` estimator (the algorithm) to learn from the training data and consequently to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:32.955606Z",
     "start_time": "2021-03-07T19:11:32.419126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# note that we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "        \n",
    "        \n",
    "als = ALS(maxIter=5, regParam=0.01, \n",
    "          userCol=user_col, \n",
    "          itemCol=item_col, \n",
    "          ratingCol=rating_col,\n",
    "          coldStartStrategy=\"drop\",\n",
    "          implicitPrefs=True\n",
    "         )\n",
    "\n",
    "# if the rating matrix is derived from another source of information\n",
    "# (i.e. it is inferred from other signals), we may set implicitPrefs\n",
    "# to True to get better results (see ALS reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipeline configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:32.970301Z",
     "start_time": "2021-03-07T19:11:32.967223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The pipeline holds two stages set above\n",
    "\n",
    "# As we will see below, we are going to use it just for evaluation purposes\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, als])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "Get the model (as transformer) by fitting the pipeline to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 15:57:52 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:52 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:53 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:54 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:55 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:55 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:56 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:57 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/04/07 15:57:57 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/04/07 15:57:57 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:57 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:58 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "22/04/07 15:57:58 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:57:59 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:00 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:00 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:01 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:01 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:02 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:02 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:03 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:04 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:04 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:05 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:06 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:06 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:07 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:07 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:08 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:09 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 15:58:09 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "Let us evaluate the ALS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "It is time to apply the model built to test data. Again, we will use the pipeline set above. Notice that, since the pipeline model is a transformer, we can easily apply it to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:33.280981Z",
     "start_time": "2021-03-07T19:11:32.971571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data and show values of columns of interest\n",
    "\n",
    "df_prediction = pipeline_model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: integer (nullable = true)\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Rating: integer (nullable = true)\n",
      " |-- ISBN-Index: double (nullable = false)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:00:29 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:30 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:30 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:31 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/04/07 16:00:32 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+----------+------------+\n",
      "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction  |\n",
      "+-------+----------+-----------+----------+------------+\n",
      "|1075   |0385504209|7          |2.0       |0.53193474  |\n",
      "|7915   |0385504209|10         |2.0       |0.41258138  |\n",
      "|24921  |0385504209|0          |2.0       |0.005520072 |\n",
      "|30261  |0385504209|0          |2.0       |0.022243943 |\n",
      "|1211   |0385504209|9          |2.0       |-0.050518215|\n",
      "|6535   |0385504209|10         |2.0       |0.0         |\n",
      "|8870   |0385504209|7          |2.0       |0.070224695 |\n",
      "|25695  |0385504209|0          |2.0       |0.003437465 |\n",
      "|3363   |0385504209|0          |2.0       |0.1754521   |\n",
      "|4169   |0385504209|10         |2.0       |0.017216714 |\n",
      "|10447  |0385504209|0          |2.0       |0.33469224  |\n",
      "|14363  |0385504209|0          |2.0       |5.597535E-4 |\n",
      "|28705  |0385504209|10         |2.0       |-0.08276379 |\n",
      "|29025  |0385504209|0          |2.0       |0.009030023 |\n",
      "|4098   |0385504209|10         |2.0       |0.039262358 |\n",
      "|11676  |0385504209|9          |2.0       |0.94296074  |\n",
      "|24082  |0385504209|9          |2.0       |1.4425745E-5|\n",
      "|27647  |0385504209|3          |2.0       |0.20653512  |\n",
      "|18167  |0385504209|0          |2.0       |-0.101063125|\n",
      "|24325  |0385504209|8          |2.0       |0.113177545 |\n",
      "+-------+----------+-----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:00:33 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:33 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:34 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:34 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/04/07 16:00:35 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174580"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking its schema and content\n",
    "\n",
    "df_prediction.printSchema()\n",
    "df_prediction.show(truncate=False)\n",
    "df_prediction.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:00:43 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:43 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:44 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:00:45 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/04/07 16:00:45 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+----------+--------------+\n",
      "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction    |\n",
      "+-------+----------+-----------+----------+--------------+\n",
      "|8      |0399135782|0          |3484.0    |-9.239088E-10 |\n",
      "|8      |0002005018|5          |16116.0   |-7.1666795E-10|\n",
      "|8      |1881320189|7          |110404.0  |3.2457742E-10 |\n",
      "|8      |0671870432|0          |193674.0  |-1.0853447E-9 |\n",
      "|10     |1841721522|0          |2993.0    |-6.372071E-13 |\n",
      "|14     |0971880107|0          |0.0       |-0.0017589737 |\n",
      "|17     |0553278398|0          |1879.0    |0.0015439332  |\n",
      "|32     |038078243X|0          |19936.0   |1.3788584E-5  |\n",
      "|39     |0553582909|8          |4640.0    |0.0022166152  |\n",
      "|44     |0842342702|0          |513.0     |0.0022894237  |\n",
      "|67     |042511774X|0          |306.0     |8.9401875E-11 |\n",
      "|99     |0451166892|3          |131.0     |-0.0023356518 |\n",
      "|99     |0446677450|10         |1380.0    |0.0011901124  |\n",
      "|99     |0312261594|8          |6614.0    |2.1523234E-4  |\n",
      "|99     |0553347594|9          |40805.0   |2.7517884E-4  |\n",
      "|114    |0446608653|9          |1096.0    |0.010439518   |\n",
      "|114    |0446612618|8          |6308.0    |-0.0067937085 |\n",
      "|160    |9728579225|8          |297234.0  |6.8620903E-10 |\n",
      "|165    |0061099325|4          |1408.0    |-0.0012133152 |\n",
      "|183    |9724129411|9          |296747.0  |5.9476557E-10 |\n",
      "+-------+----------+-----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show predictions ordered by USER-ID\n",
    "\n",
    "df_prediction.orderBy(\"User-ID\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:01:16 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:01:16 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:01:17 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:01:18 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/04/07 16:01:18 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+----------+-------------+\n",
      "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction   |\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "|16783  |0971880107|0          |0.0       |-8.065397E-9 |\n",
      "|21484  |0971880107|0          |0.0       |0.1654366    |\n",
      "|13722  |0971880107|0          |0.0       |0.012246998  |\n",
      "|26525  |0971880107|0          |0.0       |-0.01106203  |\n",
      "|17842  |0971880107|0          |0.0       |0.0          |\n",
      "|27740  |0971880107|0          |0.0       |-0.03477689  |\n",
      "|7286   |0971880107|0          |0.0       |0.4070429    |\n",
      "|10338  |0971880107|0          |0.0       |-0.0019200931|\n",
      "|17507  |0971880107|0          |0.0       |0.028336635  |\n",
      "|276925 |0971880107|0          |0.0       |-0.052555516 |\n",
      "|1025   |0971880107|0          |0.0       |0.17112182   |\n",
      "|25890  |0971880107|0          |0.0       |-0.07904161  |\n",
      "|26813  |0971880107|0          |0.0       |-0.048056927 |\n",
      "|35513  |0971880107|8          |0.0       |-1.439756E-7 |\n",
      "|6251   |0971880107|0          |0.0       |0.10822722   |\n",
      "|14     |0971880107|0          |0.0       |-0.0017589737|\n",
      "|9381   |0971880107|0          |0.0       |0.1319423    |\n",
      "|11724  |0971880107|0          |0.0       |0.036428273  |\n",
      "|16999  |0971880107|0          |0.0       |-0.1273171   |\n",
      "|33816  |0971880107|0          |0.0       |0.20483628   |\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show predictions ordered by ISBN-Index\n",
    "\n",
    "df_prediction.orderBy(\"ISBN-Index\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "Let us use an evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:02:25 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:02:25 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:02:26 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:02:26 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/04/07 16:02:27 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 4.65814551083299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:02:28 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=rating_col,\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(df_prediction)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save the pipeline for further use should it be required\n",
    "\n",
    "pipeline.save(\"pipeline-ALS\")\n",
    "\n",
    "# later on, it can be loaded anywhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 118072\r\n",
      "drwxrwxr-x 11 saltedcookie saltedcookie     4096 abr  7 16:02 .\r\n",
      "drwxrwxr-x  8 saltedcookie saltedcookie     4096 abr  7 14:41 ..\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie     4096 abr  7 15:50 books.parquet\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie 30682276 out 11  2004 BX-Book-Ratings.csv\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie 77787439 out 11  2004 BX-Books.csv\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie 12284157 out 11  2004 BX-Users.csv\r\n",
      "drwxrwxr-x  3 saltedcookie saltedcookie     4096 abr  6 20:16 data\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie      711 abr  7 15:53 derby.log\r\n",
      "drwxrwxr-x  2 saltedcookie saltedcookie     4096 abr  7 14:44 .ipynb_checkpoints\r\n",
      "drwxrwxr-x  5 saltedcookie saltedcookie     4096 abr  7 15:53 metastore_db\r\n",
      "drwxr-xr-x  4 saltedcookie saltedcookie     4096 abr  7 16:02 pipeline-ALS\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie     4096 abr  7 15:50 ratings.parquet\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie    88137 abr  7 16:02 Recommender.ipynb\r\n",
      "drwxr-xr-x  5 saltedcookie saltedcookie     4096 abr  7 15:53 spark-warehouse\r\n",
      "drwxrwxr-x  4 saltedcookie saltedcookie     4096 abr  7 14:41 .svn\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie     4096 abr  7 15:50 users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "drwxr-xr-x  4 saltedcookie saltedcookie 4096 abr  7 16:02 .\r\n",
      "drwxrwxr-x 11 saltedcookie saltedcookie 4096 abr  7 16:02 ..\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie 4096 abr  7 16:02 metadata\r\n",
      "drwxr-xr-x  4 saltedcookie saltedcookie 4096 abr  7 16:02 stages\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la pipeline-ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing recommendations and storing as persistent tables\n",
    "\n",
    "The `ALS` algorithm provides some functions to get recommendations directly. \n",
    "\n",
    "Although we can achieve results if working with predictions after the pipeline set (see below), we will take advantage of such methods directly. We should emphasize that, as it stands, we will not be using the pipeline for this task.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+-------+----------+-----------+----------+-------------+\n",
    "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction   |\n",
    "+-------+----------+-----------+----------+-------------+\n",
    "|30261  |0385504209|0          |2.0       |0.04549066   |\n",
    "|3363   |0385504209|0          |2.0       |0.03916065   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:04:28 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:28 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:29 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:29 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:30 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:30 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:31 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:32 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:32 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:33 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:34 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:34 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:35 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:36 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:36 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:37 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:37 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:38 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:39 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:39 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:40 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:40 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:41 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:41 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:42 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:43 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:43 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:04:44 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking with training data for the sake of example\n",
    "\n",
    "df_train_indexed = indexer.fit(df_train).transform(df_train)\n",
    "model = als.fit(df_train_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all distinct users and books\n",
    "\n",
    "#user_col = \"User-ID\"\n",
    "#item_col = \"ISBN-Index\" \n",
    "#rating_col = \"Book-Rating\"\n",
    "\n",
    "users = df_train_indexed.select(als.getUserCol()).distinct()\n",
    "\n",
    "books = df_train_indexed.select(als.getItemCol()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User-ID|\n",
      "+-------+\n",
      "|      2|\n",
      "|     10|\n",
      "|     14|\n",
      "|     22|\n",
      "|     38|\n",
      "|     73|\n",
      "|     88|\n",
      "|     97|\n",
      "|    102|\n",
      "|    183|\n",
      "|    232|\n",
      "|    233|\n",
      "|    254|\n",
      "|    257|\n",
      "|    280|\n",
      "|    300|\n",
      "|    313|\n",
      "|    326|\n",
      "|    343|\n",
      "|    372|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:04:58 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|ISBN-Index|\n",
      "+----------+\n",
      "|  203942.0|\n",
      "|   32738.0|\n",
      "|  104360.0|\n",
      "|   26850.0|\n",
      "|      33.0|\n",
      "|     276.0|\n",
      "|  110349.0|\n",
      "|   69773.0|\n",
      "|   10573.0|\n",
      "|    2652.0|\n",
      "|    4779.0|\n",
      "|     179.0|\n",
      "|   22501.0|\n",
      "|   22925.0|\n",
      "|    8081.0|\n",
      "|   10136.0|\n",
      "|    7620.0|\n",
      "|    1093.0|\n",
      "|  241294.0|\n",
      "|   88888.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:04:59 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    }
   ],
   "source": [
    "books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:05:08 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/04/07 16:05:09 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[92940, 298850]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[users.count(), books.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top book recommendations for users\n",
    "\n",
    "top_n_books = 2\n",
    "user_recs = model.recommendForAllUsers(top_n_books)\n",
    "\n",
    "# Generate top book recommendations for a specified set of users\n",
    "\n",
    "# subset_users = users.limit(5)\n",
    "# user_subset_recs = model.recommendForUserSubset(subset_users, top_n_books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:07:02 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n",
      "22/04/07 16:07:44 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:07:44 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:07:45 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:07:45 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:07:56 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "[Stage 620:=====================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------+\n",
      "|User-ID|recommendations                          |\n",
      "+-------+-----------------------------------------+\n",
      "|2      |[{40960, 0.0}, {40970, 0.0}]             |\n",
      "|10     |[{41, 2.3914143E-10}, {3, 2.3877048E-10}]|\n",
      "|14     |[{3, 0.004993719}, {10, 0.0046008783}]   |\n",
      "|22     |[{95, 1.4262591E-7}, {5, 1.372157E-7}]   |\n",
      "|38     |[{40960, 0.0}, {40970, 0.0}]             |\n",
      "|73     |[{1, 0.011921989}, {3, 0.011310344}]     |\n",
      "|88     |[{0, 1.0568146E-9}, {44, 8.702053E-10}]  |\n",
      "|97     |[{5, 0.061778117}, {6, 0.04637696}]      |\n",
      "|102    |[{2, 1.2556091E-9}, {1, 1.2465109E-9}]   |\n",
      "|183    |[{41, 0.039040785}, {10, 0.03275322}]    |\n",
      "|232    |[{12, 0.0150995925}, {7, 0.014233154}]   |\n",
      "|233    |[{42, 6.931729E-4}, {26, 6.8066997E-4}]  |\n",
      "|254    |[{10, 0.79451275}, {58, 0.7629388}]      |\n",
      "|257    |[{2, 0.009775513}, {6, 0.009003443}]     |\n",
      "|280    |[{4, 0.24321349}, {38, 0.23562188}]      |\n",
      "|300    |[{51, 0.013756313}, {217, 0.0120571}]    |\n",
      "|313    |[{11, 2.9560097E-9}, {71, 2.2593787E-9}] |\n",
      "|326    |[{32, 0.010707691}, {33, 0.009589998}]   |\n",
      "|343    |[{40960, 0.0}, {40970, 0.0}]             |\n",
      "|372    |[{10, 0.008792406}, {73, 0.0074664773}]  |\n",
      "+-------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:08:34 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs.show(truncate=False)\n",
    "\n",
    "# user_subset_recs.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top user recommendations for each book\n",
    "\n",
    "top_n_users = 2\n",
    "book_recs = model.recommendForAllItems(top_n_users)\n",
    "\n",
    "# Generate top user recommendations for a specified set of books\n",
    "\n",
    "# subset_books = books.limit(5)\n",
    "# book_subset_recs = model.recommendForItemSubset(subset_books, top_n_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:08:35 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n",
      "22/04/07 16:09:29 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:09:30 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:09:42 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:09:54 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:09:54 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:06 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:06 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:06 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:07 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:07 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:07 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:08 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:10:14 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------------------+\n",
      "|ISBN-Index|recommendations                            |\n",
      "+----------+-------------------------------------------+\n",
      "|2         |[{150979, 1.8297814}, {235935, 1.4957063}] |\n",
      "|4         |[{31826, 1.5439471}, {210485, 1.2995573}]  |\n",
      "|5         |[{95359, 1.5803725}, {11676, 1.1897808}]   |\n",
      "|10        |[{257204, 1.4473798}, {101851, 1.4195017}] |\n",
      "|14        |[{104636, 1.2325929}, {153662, 1.2051373}] |\n",
      "|18        |[{98391, 2.2237828}, {153662, 1.4888229}]  |\n",
      "|22        |[{11676, 1.5215749}, {257204, 1.157774}]   |\n",
      "|25        |[{254206, 1.3671017}, {31826, 1.3140694}]  |\n",
      "|38        |[{257204, 1.7772617}, {31826, 1.6275964}]  |\n",
      "|45        |[{104636, 0.84971803}, {95359, 0.8088171}] |\n",
      "|46        |[{78553, 1.2834195}, {104636, 1.1670681}]  |\n",
      "|50        |[{181687, 0.78430915}, {25409, 0.73104495}]|\n",
      "|59        |[{78973, 1.0982283}, {225087, 0.9840893}]  |\n",
      "|60        |[{25409, 1.0228671}, {56959, 0.952983}]    |\n",
      "|73        |[{100906, 1.3305316}, {257204, 1.2492762}] |\n",
      "|88        |[{95359, 1.0576013}, {11676, 0.98021734}]  |\n",
      "|90        |[{76499, 1.3779858}, {204864, 1.2040932}]  |\n",
      "|97        |[{78973, 0.7851979}, {11676, 0.7774316}]   |\n",
      "|102       |[{31315, 0.9054713}, {11676, 0.8087573}]   |\n",
      "|108       |[{11676, 0.86953366}, {204864, 0.85325605}]|\n",
      "+----------+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_recs.show(truncate=False)\n",
    "\n",
    "# book_subset_recs.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recommendations as persistent tables into the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:16:12 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/04/07 16:16:13 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n",
      "22/04/07 16:16:33 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:16:33 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:16:35 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:16:45 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:16:46 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:17:41 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:17:41 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:17:42 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:17:42 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:17:48 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "22/04/07 16:17:48 ERROR Executor: Exception in task 4.0 in stage 751.0 (TID 1799)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/04/07 16:17:48 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:17:48 ERROR Executor: Exception in task 3.0 in stage 751.0 (TID 1798)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/04/07 16:17:48 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 751.0 (TID 1798),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5723,512s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 751.0 (TID 1799): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[5723,519s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 751.0 (TID 1798): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[5723,522s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 751.0 (TID 1798): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:17:48 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 751.0 (TID 1799),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/04/07 16:17:48 WARN TaskSetManager: Lost task 3.0 in stage 751.0 (TID 1798) (TitiesBEAST.Home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "22/04/07 16:17:48 ERROR TaskSetManager: Task 3 in stage 751.0 failed 1 times; aborting job\n",
      "22/04/07 16:17:48 ERROR FileFormatWriter: Aborting job c0927943-7952-4510-81d8-9fc219542d55.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 751.0 failed 1 times, most recent failure: Lost task 3.0 in stage 751.0 (TID 1798) (TitiesBEAST.Home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:663)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "22/04/07 16:17:48 WARN TaskSetManager: Lost task 0.0 in stage 751.0 (TID 1795) (TitiesBEAST.Home executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/07 16:17:48 WARN TaskSetManager: Lost task 1.0 in stage 751.0 (TID 1796) (TitiesBEAST.Home executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/07 16:17:48 WARN TaskSetManager: Lost task 5.0 in stage 751.0 (TID 1800) (TitiesBEAST.Home executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/07 16:17:48 WARN TaskSetManager: Lost task 2.0 in stage 751.0 (TID 1797) (TitiesBEAST.Home executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o726.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:663)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 751.0 failed 1 times, most recent failure: Lost task 3.0 in stage 751.0 (TID 1798) (TitiesBEAST.Home executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n\t... 44 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_recs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUserRecommendationsTable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:806\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m--> 806\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o726.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:663)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 751.0 failed 1 times, most recent failure: Lost task 3.0 in stage 751.0 (TID 1798) (TitiesBEAST.Home executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n\t... 44 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    " user_recs.write.mode(\"overwrite\").saveAsTable(\"UserRecommendationsTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 16:11:52 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n",
      "22/04/07 16:12:01 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:12:25 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:12:37 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:12:38 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:12:39 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "22/04/07 16:13:32 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "22/04/07 16:13:36 WARN HiveExternalCatalog: Could not persist `default`.`bookrecommendationstable` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
      "org.apache.spark.SparkException: Cannot recognize hive type string: array<struct<User-ID:int,rating:float>>, column: recommendations\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotRecognizeHiveTypeError(QueryExecutionErrors.scala:1291)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.getSparkSQLDataType(HiveClientImpl.scala:1018)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.$anonfun$verifyColumnDataType$1(HiveClientImpl.scala:1033)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$verifyColumnDataType(HiveClientImpl.scala:1033)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:553)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:508)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:397)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:274)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.sql.catalyst.parser.ParseException: \n",
      "extraneous input '-' expecting {':', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 17)\n",
      "\n",
      "== SQL ==\n",
      "array<struct<User-ID:int,rating:float>>\n",
      "-----------------^^^\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseDataType(ParseDriver.scala:40)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.getSparkSQLDataType(HiveClientImpl.scala:1015)\n",
      "\t... 64 more\n"
     ]
    }
   ],
   "source": [
    "book_recs.write.mode(\"overwrite\").saveAsTable(\"BookRecommendationsTable\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It looks like we had a problem storing array<struct<ISBN-Index:int,rating:float>>\n",
    "\n",
    "Exercise: how to sort it out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 118108\r\n",
      "drwxrwxr-x 11 saltedcookie saltedcookie     4096 abr  7 16:14 .\r\n",
      "drwxrwxr-x  8 saltedcookie saltedcookie     4096 abr  7 14:41 ..\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie     4096 abr  7 15:50 books.parquet\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie 30682276 out 11  2004 BX-Book-Ratings.csv\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie 77787439 out 11  2004 BX-Books.csv\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie 12284157 out 11  2004 BX-Users.csv\r\n",
      "drwxrwxr-x  3 saltedcookie saltedcookie     4096 abr  6 20:16 data\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie      711 abr  7 15:53 derby.log\r\n",
      "drwxrwxr-x  2 saltedcookie saltedcookie     4096 abr  7 14:44 .ipynb_checkpoints\r\n",
      "drwxrwxr-x  5 saltedcookie saltedcookie     4096 abr  7 15:53 metastore_db\r\n",
      "drwxr-xr-x  4 saltedcookie saltedcookie     4096 abr  7 16:02 pipeline-ALS\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie     4096 abr  7 15:50 ratings.parquet\r\n",
      "-rw-rw-r--  1 saltedcookie saltedcookie   126027 abr  7 16:14 Recommender.ipynb\r\n",
      "drwxr-xr-x  7 saltedcookie saltedcookie     4096 abr  7 16:11 spark-warehouse\r\n",
      "drwxrwxr-x  4 saltedcookie saltedcookie     4096 abr  7 14:41 .svn\r\n",
      "drwxr-xr-x  2 saltedcookie saltedcookie     4096 abr  7 15:50 users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring results\n",
    "1. Given a user, shows the recommended list of books.\n",
    "2. Given a book, shows the list of users who might be interested on.\n",
    "\n",
    "We are going to use Spark SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user to explore\n",
    "\n",
    "user = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book to explore\n",
    "\n",
    "book = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us check the SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register information about users as a SQL temporary view\n",
    "\n",
    "df_clean_users.createOrReplaceTempView(\"users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register information about books as a SQL temporary view\n",
    "\n",
    "df_clean_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark.catalog.listTables(dbName=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use managed tables\n",
    "\n",
    "spark.sql(\"USE default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listColumns('bookstable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark.sql(\"SELECT * FROM users\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark.sql(\"SELECT * FROM books\").show(10, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"The recommended books for user \" + str(user) + \" are: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it as exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"The users who might be interested on the book \" + str(book) + \" are: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We leave it as exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model\n",
    "\n",
    "We can improve the model. For example, by carrying out better data cleasing operations and take into consideration efficiency issues. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Additional exercise\n",
    "\n",
    "Given the current status of this notebook, redo its content such that major tasks are split into \n",
    "various notebooks, ou Python modules. \n",
    "The purpose is to modularize code having in mind the setup of a real recommender system. That is:\n",
    "- A downloader module, focussing on downloading data, cleasing it, and then storing it in a data store.\n",
    "- A recommender module, to create a recommendation module and to pre-compute recommendations in order to save them a data store.\n",
    "- A recommender server, to retrieve recommendations upon queries made to the data store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Learning Spark - Lightning-Fast Data Analytics, 2nd Ed. J. Damji, B. Wenig, T. Das, and D. Lee. O'Reilly, 2020\n",
    "* Spark: The Definitive Guide - Big Data Processing Made Simple, 1st Ed. B. Chambers and M. Zaharia. O'Reilly, 2018\n",
    "* http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://docs.python.org/3/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "249px",
    "width": "332px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.98px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
